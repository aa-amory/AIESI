{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIESI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZywuWnSGi1j",
        "outputId": "80785d27-f14c-4a3b-dc16-375b8e200f8f"
      },
      "source": [
        "!git pull https://github.com/aa-amory/AIESI.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqRwxlrgIbPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58c7157-b775-41c9-f201-96da00130574"
      },
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import numpy as np\n",
        "!pip install opencv-wrapper\n",
        "import opencv_wrapper as cvw\n",
        "from skimage.filters import threshold_local\n",
        "import json\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-wrapper in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: numpy<=1.16.2 in /usr/local/lib/python3.7/dist-packages (from opencv-wrapper) (1.16.2)\n",
            "Requirement already satisfied: opencv-python<=4.0.1 in /usr/local/lib/python3.7/dist-packages (from opencv-wrapper) (4.0.0.21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESOTJjyly1Oy"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install -U git+https://github.com/madmaze/pytesseract.git\n",
        "\n",
        "import pytesseract\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install colorama"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzyjJFXwAYAm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jErwKuqjy-C3"
      },
      "source": [
        "\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required = True, help = \"Path to the image to be scanned\")\n",
        "# ap.add_argument(\"-o\", \"--output\", required = True, help = \"Path to the image to be scanned\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "\n",
        "def get_acc(directory, path):\n",
        "    font     = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    fontScale = 0.5\n",
        "    fontColor  = (255,0,0)\n",
        "    lineType = 1\n",
        "    path = directory+path\n",
        "    # path = args['image']\n",
        "    # op_path = args['output']\n",
        "\n",
        "    op_path = directory\n",
        "    if op_path[-1]!='/':\n",
        "    \top_path.append('/')\n",
        "\n",
        "\n",
        "    #Threshold\n",
        "    image = cv2.imread(path)\n",
        "\n",
        "    height,width,channel = image.shape\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    T = threshold_local(gray, 15, offset = 6, method = \"gaussian\") # generic, mean, median, gaussian\n",
        "    thresh = (gray > T).astype(\"uint8\") * 255\n",
        "    thresh = ~thresh\n",
        "\n",
        "    #cv2.imwrite(op_path+'threshold.png', thresh)\n",
        "\n",
        "    #Dilation\n",
        "    kernel =np.ones((1,1), np.uint8)\n",
        "    ero = cv2.erode(thresh, kernel, iterations= 1)\n",
        "    img_dilation = cv2.dilate(ero, kernel, iterations=1)\n",
        "    #cv2.imwrite(op_path+'dilated.png', img_dilation)\n",
        "\n",
        "    # Remove noise\n",
        "    nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(img_dilation, None, None, None, 8, cv2.CV_32S)\n",
        "    sizes = stats[1:, -1] #get CC_STAT_AREA component\n",
        "    final = np.zeros((labels.shape), np.uint8)\n",
        "    for i in range(0, nlabels - 1):\n",
        "        if sizes[i] >= 10:   #filter small dotted regions\n",
        "            final[labels == i + 1] = 255\n",
        "    #cv2.imwrite(op_path+'final.png', final)\n",
        "    #Find contours\n",
        "    kern = np.ones((5,15), np.uint8)\n",
        "    img_dilation = cv2.dilate(final, kern, iterations = 1)\n",
        "    #cv2.imwrite(op_path+'contours.png', img_dilation)\n",
        "    contours, hierarchy = cv2.findContours(img_dilation, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "    # Map contours to bounding rectangles, using bounding_rect property\n",
        "    rects = map(lambda c: cv2.boundingRect(c), contours)\n",
        "    # Sort rects by top-left x (rect.x == rect.tl.x)\n",
        "    sorted_rects = sorted(rects, key =lambda r: r[0])\n",
        "    sorted_rects = sorted(sorted_rects, key =lambda r: r[1])\n",
        "\n",
        "    tt = image.copy()\n",
        "    dictionary = {}\n",
        "    etfo=''\n",
        "    for i,rect in enumerate(sorted_rects):\n",
        "        temp_dic = {}\n",
        "        x,y,w,h = rect\n",
        "        if(w<20 or h<20):\n",
        "            continue\n",
        "        temp_dic['coords'] = [x,y,w,h]\n",
        "        words = []\n",
        "        temp = tt[y:y+h, x:x+w]\n",
        "        #cv2.imwrite('/content/gdrive/My Drive/ResearchPaper/AIESI_complete_pipeline/temp/'+str(i)+'.png',temp)\n",
        "        temp = cv2.cvtColor(temp, cv2.COLOR_BGR2RGB)\n",
        "        hi = pytesseract.image_to_data(temp, config=r'--psm 6')\n",
        "        hi = hi.split()\n",
        "        ind = 22\n",
        "        while(True):\n",
        "            if (ind>len(hi)):\n",
        "                break\n",
        "            if(int(hi[ind])==-1):\n",
        "                ind+=11\n",
        "            else:\n",
        "                #cv2.putText(image,hi[ind]+','+hi[ind+1], (x,y), font, fontScale,fontColor,lineType)\n",
        "                tem = {}\n",
        "                tem['confidence'] = hi[ind]\n",
        "                tem['text'] = hi[ind+1]\n",
        "                etfo=etfo+hi[ind+1]\n",
        "                etfo=etfo+\" \"\n",
        "                x+=len(hi[ind+1])*20\n",
        "                ind+=12\n",
        "                words.append(tem)\n",
        "        temp_dic['words'] = words\n",
        "        etfo=etfo+'\\n'\n",
        "        #cvw.rectangle(image, rect, cvw.Color.GREEN, thickness=1)\n",
        "        dictionary[i] = temp_dic\n",
        "\n",
        "\n",
        "    cv2.imwrite(op_path+\"result.png\", image)\n",
        "    return json.dumps(dictionary),etfo\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2aVUPge8y0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8625a770-24ad-4342-ec68-5d034e6b9962"
      },
      "source": [
        "\n",
        "gson_data, etfo = get_acc('/content/AIESI/dataset/','1013-receipt.jpg')\n",
        "print(gson_data)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"1\": {\"coords\": [259, 104, 128, 44], \"words\": [{\"confidence\": \"92\", \"text\": \"ALBETOS\"}]}, \"4\": {\"coords\": [413, 143, 143, 45], \"words\": [{\"confidence\": \"95\", \"text\": \"FOOD\"}]}, \"5\": {\"coords\": [146, 152, 240, 51], \"words\": [{\"confidence\": \"94\", \"text\": \"MEXICAN\"}]}, \"17\": {\"coords\": [429, 195, 92, 40], \"words\": [{\"confidence\": \"12\", \"text\": \"BLVD.\"}]}, \"18\": {\"coords\": [297, 199, 127, 43], \"words\": [{\"confidence\": \"96\", \"text\": \"ARTESIA\"}]}, \"19\": {\"coords\": [201, 206, 92, 40], \"words\": [{\"confidence\": \"96\", \"text\": \"11732\"}]}, \"24\": {\"coords\": [236, 248, 186, 44], \"words\": [{\"confidence\": \"88\", \"text\": \"ARTESIA,CA.\"}]}, \"26\": {\"coords\": [366, 290, 143, 41], \"words\": [{\"confidence\": \"37\", \"text\": \"860-2530\"}]}, \"27\": {\"coords\": [273, 295, 86, 44], \"words\": [{\"confidence\": \"96\", \"text\": \"(562)\"}]}, \"28\": {\"coords\": [190, 301, 73, 38], \"words\": [{\"confidence\": \"95\", \"text\": \"Ph:\"}]}, \"32\": {\"coords\": [400, 334, 143, 41], \"words\": [{\"confidence\": \"96\", \"text\": \"WELCOME\"}]}, \"33\": {\"coords\": [91, 337, 34, 35], \"words\": [{\"confidence\": \"43\", \"text\": \"a\"}]}, \"34\": {\"coords\": [273, 341, 123, 38], \"words\": [{\"confidence\": \"95\", \"text\": \"ORDERS\"}]}, \"35\": {\"coords\": [164, 346, 107, 38], \"words\": [{\"confidence\": \"96\", \"text\": \"PHONE\"}]}, \"38\": {\"coords\": [293, 427, 167, 39], \"words\": [{\"confidence\": \"87\", \"text\": \"01029\"}]}, \"39\": {\"coords\": [45, 435, 166, 38], \"words\": [{\"confidence\": \"96\", \"text\": \"ORDER\"}]}, \"40\": {\"coords\": [231, 435, 40, 34], \"words\": [{\"confidence\": \"89\", \"text\": \"#\"}]}, \"55\": {\"coords\": [569, 512, 81, 37], \"words\": [{\"confidence\": \"72\", \"text\": \"8,10\"}]}, \"56\": {\"coords\": [218, 520, 74, 35], \"words\": [{\"confidence\": \"96\", \"text\": \"TACO\"}]}, \"57\": {\"coords\": [124, 521, 91, 36], \"words\": [{\"confidence\": \"91\", \"text\": \"ASADA\"}]}, \"58\": {\"coords\": [91, 523, 28, 34], \"words\": [{\"confidence\": \"95\", \"text\": \"3\"}]}, \"59\": {\"coords\": [571, 558, 81, 37], \"words\": [{\"confidence\": \"69\", \"text\": \"0,75\"}]}, \"60\": {\"coords\": [187, 563, 105, 36], \"words\": [{\"confidence\": \"96\", \"text\": \"CHARGE\"}]}, \"61\": {\"coords\": [124, 564, 60, 36], \"words\": [{\"confidence\": \"96\", \"text\": \"ATM\"}]}, \"63\": {\"coords\": [123, 650, 138, 37], \"words\": [{\"confidence\": \"96\", \"text\": \"SUBTOTAL\"}]}, \"64\": {\"coords\": [456, 650, 28, 40], \"words\": [{\"confidence\": \"92\", \"text\": \"$\"}]}, \"65\": {\"coords\": [574, 652, 81, 36], \"words\": [{\"confidence\": \"76\", \"text\": \"8.85\"}]}, \"66\": {\"coords\": [186, 694, 91, 38], \"words\": [{\"confidence\": \"96\", \"text\": \"TOTAL\"}]}, \"67\": {\"coords\": [122, 695, 60, 37], \"words\": [{\"confidence\": \"96\", \"text\": \"TAX\"}]}, \"68\": {\"coords\": [457, 696, 28, 40], \"words\": [{\"confidence\": \"93\", \"text\": \"$\"}]}, \"69\": {\"coords\": [576, 698, 81, 39], \"words\": [{\"confidence\": \"15\", \"text\": \"ORS\"}]}, \"71\": {\"coords\": [121, 740, 92, 38], \"words\": [{\"confidence\": \"96\", \"text\": \"TOTAL\"}]}, \"72\": {\"coords\": [458, 743, 29, 40], \"words\": [{\"confidence\": \"91\", \"text\": \"$\"}]}, \"73\": {\"coords\": [577, 746, 82, 38], \"words\": [{\"confidence\": \"80\", \"text\": \"9\"}, {\"confidence\": \"80\", \"text\": \"58\"}]}, \"75\": {\"coords\": [119, 832, 61, 39], \"words\": [{\"confidence\": \"96\", \"text\": \"ATM\"}]}, \"76\": {\"coords\": [461, 837, 28, 41], \"words\": [{\"confidence\": \"89\", \"text\": \"$\"}]}, \"77\": {\"coords\": [580, 842, 82, 40], \"words\": [{\"confidence\": \"68\", \"text\": \"9.458\"}]}, \"78\": {\"coords\": [67, 926, 112, 40], \"words\": [{\"confidence\": \"96\", \"text\": \"RECALL\"}]}, \"79\": {\"coords\": [269, 927, 72, 37], \"words\": [{\"confidence\": \"29\", \"text\": \"\\u2018636\"}]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-pl_BhaGlVT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgPaAy8tzHyq"
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import json\n",
        "from torch import nn"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsDHbMbczUlM"
      },
      "source": [
        "class MyModel0(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_size * 2, 5)\n",
        "\n",
        "    def forward(self, inpt):\n",
        "        embedded = self.embed(inpt)\n",
        "        feature, _ = self.lstm(embedded)\n",
        "        oupt = self.linear(feature)\n",
        "        return oupt\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbs9mVYOziyI"
      },
      "source": [
        "### Hyper- parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3YGzBc9zh9g"
      },
      "source": [
        "device= 'cpu'\n",
        "hidden_size = 256\n",
        "\n",
        "device= torch.device('cpu')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ViR8cdl9bAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696fcb01-fcfe-4331-bee8-67acd03517f5"
      },
      "source": [
        "!pip install colorama"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq5eXyUgz48Q"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from os import path\n",
        "from string import ascii_uppercase, digits, punctuation\n",
        "\n",
        "import colorama\n",
        "import numpy\n",
        "import regex\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8y21Vcz9rg"
      },
      "source": [
        "VOCAB= ascii_uppercase+digits+punctuation+\" \\t\\n\"\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J46vPsECGmpi"
      },
      "source": [
        "print(etfo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U80WIAovZFB-"
      },
      "source": [
        "temp_text=''\n",
        "for i in range(len(etfo)):\n",
        "    temp_text=temp_text+etfo[i]\n",
        "etfo=temp_text"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvvOZvt_JNej"
      },
      "source": [
        "# conver lower text to uppper text in ETFO\n",
        "etfo= etfo.upper()\n",
        "print(etfo)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a74b3-A92YJ9"
      },
      "source": [
        "def get_test_data():\n",
        "    \n",
        "    text = etfo\n",
        "    text_tensor = torch.zeros(len(text), 1, dtype=torch.long)\n",
        "    text_tensor[:, 0] = torch.LongTensor([VOCAB.find(c) for c in text])\n",
        "\n",
        "    return text_tensor.to(device)\n",
        "\n",
        "text_tensor = get_test_data()\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5VRkcMWZNGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289d2c5d-8400-461f-a6e0-374693ca167e"
      },
      "source": [
        "print(text_tensor.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([251, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVY1sh_07jpC"
      },
      "source": [
        "def pred_to_dict(text, pred, prob):\n",
        "    res = {\"company\": (\"\", 0), \"date\": (\"\", 0), \"address\": (\"\", 0), \"total\": (\"\", 0)}\n",
        "    keys = list(res.keys())\n",
        "\n",
        "    seps = [0] + (numpy.nonzero(numpy.diff(pred))[0] + 1).tolist() + [len(pred)]\n",
        "    for i in range(len(seps) - 1):\n",
        "        pred_class = pred[seps[i]] - 1\n",
        "        if pred_class == -1:\n",
        "            continue\n",
        "\n",
        "        new_key = keys[pred_class]\n",
        "        new_prob = prob[seps[i] : seps[i + 1]].max()\n",
        "        if new_prob > res[new_key][1]:\n",
        "            res[new_key] = (text[seps[i] : seps[i + 1]], new_prob)\n",
        "\n",
        "    return {k: regex.sub(r\"[\\t\\n]\", \" \", v[0].strip()) for k, v in res.items()}\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSph1VnoV13O",
        "outputId": "8e6a0c8a-0768-4251-f7b4-196302377e67"
      },
      "source": [
        "print(text_tensor[len(text_tensor)-1])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([70])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uik49ehFV__h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9515f119-905d-42c7-ed56-d4333ab94373"
      },
      "source": [
        "print(text_tensor.shape)\n",
        "for i in range(len(text_tensor)-1):\n",
        "  if text_tensor[i]<0 or text_tensor[i]>70:\n",
        "    text_tensor = torch.cat([text_tensor[0:i], text_tensor[i+1:]])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([251, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAn7cPBhzec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec00b04c-160a-4cd0-b7b4-28858b8ff214"
      },
      "source": [
        "\n",
        "def test():\n",
        "    print('-------------------------------loged----------------------------------')\n",
        "    model = MyModel0(len(VOCAB), 16, hidden_size).to(device)\n",
        "    # dataset = MyDataset(None, args.device, test_path=\"/content/gdrive/My Drive/ResearchPaper/KIPE/data/test_dict.pth\")\n",
        "\n",
        "    model.load_state_dict(torch.load(\"/content/AIESI/AIESI_using_tesseract/model.pth\"))\n",
        "\n",
        "    model.eval()\n",
        "  \n",
        "    with torch.no_grad():\n",
        "            oupt = model(text_tensor)\n",
        "            prob = torch.nn.functional.softmax(oupt, dim=2)\n",
        "            prob, pred = torch.max(prob, dim=2)\n",
        "            prob = prob.squeeze().cpu().numpy()\n",
        "            pred = pred.squeeze().cpu().numpy()\n",
        "            real_text = etfo\n",
        "            result = pred_to_dict(real_text, pred, prob)\n",
        "\n",
        "            with open(\"/content/AIESI/AIESI_using_tesseract/results\" + 'result.txt' + \".txt\", \"w\", encoding=\"utf-8\") as json_opened:\n",
        "                json.dump(result, json_opened, indent=4)\n",
        "\n",
        "            print('-------------------------------result----------------------------------')\n",
        "            print(result)\n",
        "            #print(key)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------loged test----------------------------------\n",
            "-------------------------------result----------------------------------\n",
            "{'company': '', 'date': '', 'address': '1', 'total': '9.458'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyWX5xThdy5O"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "# This is formatted as code\r\n",
        "```\r\n",
        "\r\n",
        "# PIP all required lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgFm5iIEdZ5o"
      },
      "source": [
        "!git pull https://github.com/aa-amory/AIESI.git\r\n",
        "!pip install -U git+https://github.com/madmaze/pytesseract.git\r\n",
        "!sudo apt install tesseract-ocr\r\n",
        "!pip install colorama\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NLchCHPeO89"
      },
      "source": [
        "# full code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UecDajfpVT"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class MyModel0(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\r\n",
        "        super().__init__()\r\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\r\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=True)\r\n",
        "        self.linear = nn.Linear(hidden_size * 2, 5)\r\n",
        "\r\n",
        "    def forward(self, inpt):\r\n",
        "        embedded = self.embed(inpt)\r\n",
        "        feature, _ = self.lstm(embedded)\r\n",
        "        oupt = self.linear(feature)\r\n",
        "        return oupt\r\n",
        "\r\n",
        "def get_test_data():\r\n",
        "    \r\n",
        "    text = etfo\r\n",
        "    text_tensor = torch.zeros(len(text), 1, dtype=torch.long)\r\n",
        "    text_tensor[:, 0] = torch.LongTensor([VOCAB.find(c) for c in text])\r\n",
        "\r\n",
        "    return text_tensor.to(device)\r\n",
        "\r\n",
        "def pred_to_dict(text, pred, prob):\r\n",
        "    res = {\"company\": (\"\", 0), \"date\": (\"\", 0), \"address\": (\"\", 0), \"total\": (\"\", 0)}\r\n",
        "    keys = list(res.keys())\r\n",
        "\r\n",
        "    seps = [0] + (numpy.nonzero(numpy.diff(pred))[0] + 1).tolist() + [len(pred)]\r\n",
        "    for i in range(len(seps) - 1):\r\n",
        "        pred_class = pred[seps[i]] - 1\r\n",
        "        if pred_class == -1:\r\n",
        "            continue\r\n",
        "\r\n",
        "        new_key = keys[pred_class]\r\n",
        "        new_prob = prob[seps[i] : seps[i + 1]].max()\r\n",
        "        if new_prob > res[new_key][1]:\r\n",
        "            res[new_key] = (text[seps[i] : seps[i + 1]], new_prob)\r\n",
        "\r\n",
        "    return {k: regex.sub(r\"[\\t\\n]\", \" \", v[0].strip()) for k, v in res.items()}\r\n",
        "\r\n",
        "\r\n",
        "def test():\r\n",
        "    print('-------------------------------loged test----------------------------------')\r\n",
        "    model = MyModel0(len(VOCAB), 16, hidden_size).to(device)\r\n",
        "    # dataset = MyDataset(None, args.device, test_path=\"/content/gdrive/My Drive/ResearchPaper/KIPE/data/test_dict.pth\")\r\n",
        "\r\n",
        "    model.load_state_dict(torch.load(\"/content/AIESI/AIESI_using_tesseract/model.pth\"))\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "  \r\n",
        "    with torch.no_grad():\r\n",
        "            oupt = model(text_tensor)\r\n",
        "            prob = torch.nn.functional.softmax(oupt, dim=2)\r\n",
        "            prob, pred = torch.max(prob, dim=2)\r\n",
        "            prob = prob.squeeze().cpu().numpy()\r\n",
        "            pred = pred.squeeze().cpu().numpy()\r\n",
        "            real_text = etfo\r\n",
        "            result = pred_to_dict(real_text, pred, prob)\r\n",
        "\r\n",
        "            with open(\"/content/AIESI/AIESI_using_tesseract/results\" + 'result.txt' + \".txt\", \"w\", encoding=\"utf-8\") as json_opened:\r\n",
        "                json.dump(result, json_opened, indent=4)\r\n",
        "\r\n",
        "            print('-------------------------------result----------------------------------')\r\n",
        "            print(result)\r\n",
        "            #print(key)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkyHV8TSIVNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "698f8607-4281-4195-cce1-cc1c3efbb777"
      },
      "source": [
        "import json\r\n",
        "import os\r\n",
        "import random\r\n",
        "from os import path\r\n",
        "from string import ascii_uppercase, digits, punctuation\r\n",
        "import colorama\r\n",
        "import regex\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import pytesseract\r\n",
        "import cv2\r\n",
        "import argparse\r\n",
        "import numpy as np\r\n",
        "import opencv_wrapper as cvw\r\n",
        "from skimage.filters import threshold_local\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "gson_data, etfo = get_acc('/content/AIESI/dataset/','1014-receipt.jpg')\r\n",
        "print('----------------gson_data---------------------------')\r\n",
        "print(gson_data)\r\n",
        "\r\n",
        "device= 'cpu'\r\n",
        "hidden_size = 256\r\n",
        "device= torch.device('cpu')\r\n",
        "\r\n",
        "VOCAB= ascii_uppercase+digits+punctuation+\" \\t\\n\"\r\n",
        "temp_text=''\r\n",
        "for i in range(len(etfo)):\r\n",
        "    temp_text=temp_text+etfo[i]\r\n",
        "etfo=temp_text\r\n",
        "etfo= etfo.upper()\r\n",
        "print('--------------------etfo-----------------------')\r\n",
        "print(etfo)\r\n",
        "text_tensor = get_test_data()\r\n",
        "print('-------------------text_tensor------------------------')\r\n",
        "print(text_tensor.shape)\r\n",
        "for i in range(len(text_tensor)-1):\r\n",
        "  if text_tensor[i]<0 or text_tensor[i]>70:\r\n",
        "    text_tensor = torch.cat([text_tensor[0:i], text_tensor[i+1:]])\r\n",
        "test()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-7cfd18f123b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/AIESI/dataset/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1014-receipt.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----------------gson_data---------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-b528a5df6678>\u001b[0m in \u001b[0;36mget_acc\u001b[0;34m(directory, path)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}